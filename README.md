# ğŸ§  RAGMODEL using GROQ

A Retrieval-Augmented Generation (RAG) Chatbot system powered by the GROQ LPU backend for high-speed inference. This full-stack project allows users to upload PDFs, extract relevant data, and interact with it in a conversational format.

## ğŸš€ Features

- âš¡ High-speed inference using GROQ LPU
- ğŸ“„ PDF uploading with OCR fallback for scanned documents
- ğŸ” Chunk-based retrieval using embeddings
- ğŸ§  RAG pipeline for intelligent answers from your files
- ğŸŒ Full-stack application (FastAPI + React)
- ğŸ”’ Secure with CORS and environment variable handling

---

## ğŸ“ Project Structure

```
RAGMODEL-using-GROQ/
â”œâ”€â”€ rag-chatbot-backend/     # FastAPI backend
â”‚   â”œâ”€â”€ main.py              # FastAPI app entry point
â”‚   â”œâ”€â”€ utils/               # PDF chunking, OCR, embedding tools
â”‚   â””â”€â”€ requirements.txt     # Backend dependencies
â”œâ”€â”€ rag-chatbot-frontend/    # React frontend
â”‚   â”œâ”€â”€ App.jsx              # Main frontend logic
â”‚   â”œâ”€â”€ components/          # Sidebar, chat UI
â”‚   â””â”€â”€ vite.config.js       # Vite build config
â””â”€â”€ README.md                # Project documentation
```

---

## ğŸ§ª Tech Stack

| Layer      | Tech Used           |
|------------|---------------------|
| Frontend   | React, Vite, Tailwind CSS |
| Backend    | FastAPI, Python, Tesseract, PyMuPDF |
| NLP Engine | GROQ LPU, HuggingFace Transformers |
| DB         | ChromaDB (for vector storage) |
| OCR        | Tesseract, PyMuPDF fallback |

---

## âš™ï¸ Installation

### Backend

```bash
cd rag-chatbot-backend
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
uvicorn main:app --reload
```

### Frontend

```bash
cd rag-chatbot-frontend
npm install
npm run dev
```

---

## ğŸ” Environment Variables

Create a `.env` file in `rag-chatbot-backend/` with:

```env
OPENAI_API_KEY=your_key_here
GROQ_API_KEY=your_key_here
```

---

## ğŸ› ï¸ Usage

1. Upload PDFs via the UI.
2. The backend processes the files, chunks them, and creates embeddings.
3. Ask questions based on the documents.
4. Answers are generated using the RAG pipeline via GROQ.

---

## ğŸ“¦ Deployment

- Backend can be containerized with Docker.
- Frontend supports static hosting (e.g., Vercel, Netlify).
- Ensure GROQ keys are secure and not pushed.

---

## ğŸ™Œ Contributions

Pull requests and issues are welcome. For major changes, open an issue first to discuss what youâ€™d like to change.

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¨â€ğŸ’» Author

[Jayasudhan M](https://github.com/Jayasudhandesigner) â€“ Designer, AI Engineer, and Full-Stack Developer.
